# LLM Token Visualizer Suite

A collection of tools to visualize tokens generated by LLMs, with support for both **Ollama** and **llama.cpp**. See token generation in real-time, view probabilities, and understand how models make decisions.

## Tools Available

### 1. Space Colonization Token Visualizer ([space_colonization/](space_colonization/))
- ğŸŒ³ **Organic tree growth** - Tokens branch out using the space colonization algorithm
- ğŸ® **Interactive selection** - Navigate with arrow keys, select with space
- ğŸ‘» **Missed branches** - See alternative paths grow in the background
- ğŸ“Š **Probability bars** - Text-based `â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘` visualization
- ğŸ¨ **p5.js canvas** - Beautiful, full-screen visualization

### 2. Ollama Token Visualizer ([ollama_token_visualizer.py](ollama_token_visualizer.py))
- ğŸ¨ **Color-coded tokens** with numbers
- ğŸ“Š **Real-time streaming**
- ğŸ“ˆ **Performance statistics**
- âœ… Simple, works with existing Ollama setup

### 3. Ollama Experimental Visualizer ([ollama_token_visualizer2.py](ollama_token_visualizer2.py))
- ğŸ”¬ Tests both `/api/generate` and `/api/chat` endpoints
- ğŸ“‹ Shows all available metadata from Ollama
- ğŸ” Explores what data Ollama exposes

### 4. LLaMA.cpp Probability Visualizer ([llamacpp_probability_visualizer.py](llamacpp_probability_visualizer.py))
- ğŸ¯ **Token probabilities** - See how confident the model is
- ğŸŒ³ **Alternative tokens** - What else the model considered
- ğŸ“Š **Decision tree view** - Visualize the selection process
- ğŸ² **Full control** over sampling parameters

### 5. Interactive Token Tree Selector ([interactive_token_selector.py](interactive_token_selector.py))
- ğŸ® **Interactive mode** - YOU choose which token comes next
- ğŸŒ³ **Live path tree** - See your choices visualized as a tree
- ğŸ¨ **Perfect for installations** - Audience participation
- ğŸ“Š **Probability bars** - Visual feedback on each choice

## Quick Comparison

| Feature | Space Colonization | Ollama Visualizer | LLaMA.cpp Visualizer |
|---------|-------------------|------------------|---------------------|
| Visual tree | âœ… Organic growth | âŒ | âœ… Text-based |
| Token numbers | âŒ | âœ… | âœ… |
| Token text | âœ… | âœ… | âœ… |
| **Probabilities** | âœ… Visual bars | âŒ | âœ… |
| **Alternative tokens** | âœ… Background trees | âŒ | âœ… |
| **Interactive selection** | âœ… | âŒ | âŒ |
| Setup | Medium | Easy | Medium |

## Prerequisites

### For Space Colonization Visualizer
- Python 3.7+
- llama.cpp server running on port 8080
- A GGUF format model
- Modern web browser (for p5.js frontend)

### For Ollama Visualizers
- Python 3.7+
- Ollama installed and running
- At least one Ollama model (e.g., `mistral`, `llama3`)

### For LLaMA.cpp Visualizer (Additional)
- llama.cpp installed: `brew install llama.cpp`
- GGUF format model (can use Ollama's models)

## Installation

See [SETUP.md](SETUP.md) for detailed virtual environment setup.

### Quick Start

```bash
# Create and activate virtual environment
python3 -m venv tokenviz
source tokenviz/bin/activate

# Install dependencies
pip install -r requirements.txt
```

## Usage

### 1. Space Colonization Visualizer (Recommended)

**Step 1: Start llama.cpp server** (in a separate terminal):
```bash
llama-server -m /path/to/your/model.gguf --port 8080
```

**Step 2: Run the visualization**:
```bash
cd space_colonization
./run.sh
```

**Step 3: Open in browser**:
Navigate to `http://localhost:8001`

**Controls:**
- `â†‘` / `â†“` - Navigate between token options
- `Space` or `â†’` - Select token and grow tree
- Watch missed branches grow organically in the background!

### 2. Ollama Token Visualizer (Basic)

```bash
# Make sure Ollama is running (icon in menu bar)

# Interactive mode
python ollama_token_visualizer.py

# Direct prompt
python ollama_token_visualizer.py --prompt "Hello, world!"

# With specific model
python ollama_token_visualizer.py --model llama3:latest --prompt "Explain AI"

# List available models
python ollama_token_visualizer.py --list-models
```

### 2. LLaMA.cpp Probability Visualizer (Advanced)

**First, start the llama.cpp server:**
```bash
./start_llamacpp_server.sh
```

**Then run the visualizer:**
```bash
# Basic - shows token probabilities
python llamacpp_probability_visualizer.py --prompt "The sky is"

# Show top 10 alternatives per token
python llamacpp_probability_visualizer.py --prompt "AI is" --n-probs 10

# Decision tree view
python llamacpp_probability_visualizer.py --prompt "Once upon a time" --tree

# Adjust temperature
python llamacpp_probability_visualizer.py --prompt "Hello" --temperature 1.0
```

See [LLAMACPP_GUIDE.md](LLAMACPP_GUIDE.md) for detailed llama.cpp usage.

## Examples

```bash
# Quick question
python ollama_token_visualizer.py -p "What is the capital of France?"

# Code generation
python ollama_token_visualizer.py -m codellama -p "Write a Python function to calculate fibonacci numbers"

# Creative writing
python ollama_token_visualizer.py -m mistral -p "Tell me a short story about a robot"
```

## Output Sections

The visualizer displays four sections:

1. **PROMPT** - Your input prompt
2. **TOKENS** - Color-coded individual tokens as they stream in
3. **STATISTICS** - Performance metrics (duration, tokens/second)
4. **COMPLETE TEXT** - The final assembled text output

## Troubleshooting

### "Error connecting to Ollama"
- Make sure Ollama is running: `ollama serve`
- Check that Ollama is accessible at `http://localhost:11434`

### "No models found"
- Pull a model first: `ollama pull llama2`
- List available models: `ollama list`

### Slow generation
- Smaller models (like `llama2:7b`) are faster than larger ones
- Performance depends on your hardware (CPU/GPU)

## How It Works

The tool uses Ollama's streaming API to receive tokens in real-time. Each token (which could be a word, part of a word, or punctuation) is displayed with a unique color as it arrives, giving you insight into how the LLM constructs its response piece by piece.

## License

MIT
