# LLM Token Visualizer Suite

A collection of Python tools to visualize tokens generated by LLMs, with support for both **Ollama** and **llama.cpp**. See token generation in real-time, view probabilities, and understand how models make decisions.

## Tools Available

### 1. Ollama Token Visualizer ([ollama_token_visualizer.py](ollama_token_visualizer.py))
- üé® **Color-coded tokens** with numbers
- üìä **Real-time streaming**
- üìà **Performance statistics**
- ‚úÖ Simple, works with existing Ollama setup

### 2. Ollama Experimental Visualizer ([ollama_token_visualizer2.py](ollama_token_visualizer2.py))
- üî¨ Tests both `/api/generate` and `/api/chat` endpoints
- üìã Shows all available metadata from Ollama
- üîç Explores what data Ollama exposes

### 3. LLaMA.cpp Probability Visualizer ([llamacpp_probability_visualizer.py](llamacpp_probability_visualizer.py))
- üéØ **Token probabilities** - See how confident the model is
- üå≥ **Alternative tokens** - What else the model considered
- üìä **Decision tree view** - Visualize the selection process
- üé≤ **Full control** over sampling parameters

### 4. Interactive Token Tree Selector ([interactive_token_selector.py](interactive_token_selector.py))
- üéÆ **Interactive mode** - YOU choose which token comes next
- üå≥ **Live path tree** - See your choices visualized as a tree
- üé® **Perfect for installations** - Audience participation
- üìä **Probability bars** - Visual feedback on each choice

## Quick Comparison

| Feature | Ollama Visualizer | LLaMA.cpp Visualizer |
|---------|------------------|---------------------|
| Token numbers | ‚úÖ | ‚úÖ |
| Token text | ‚úÖ | ‚úÖ |
| Colors | ‚úÖ | ‚úÖ |
| **Probabilities** | ‚ùå | ‚úÖ |
| **Alternative tokens** | ‚ùå | ‚úÖ |
| **Decision tree** | ‚ùå | ‚úÖ |
| Setup | Easy | Medium |

## Prerequisites

### For Ollama Visualizers
- Python 3.7+
- Ollama installed and running
- At least one Ollama model (e.g., `mistral`, `llama3`)

### For LLaMA.cpp Visualizer (Additional)
- llama.cpp installed: `brew install llama.cpp`
- GGUF format model (can use Ollama's models)

## Installation

See [SETUP.md](SETUP.md) for detailed virtual environment setup.

### Quick Start

```bash
# Create and activate virtual environment
python3 -m venv tokenviz
source tokenviz/bin/activate

# Install dependencies
pip install -r requirements.txt
```

## Usage

### 1. Ollama Token Visualizer (Basic)

```bash
# Make sure Ollama is running (icon in menu bar)

# Interactive mode
python ollama_token_visualizer.py

# Direct prompt
python ollama_token_visualizer.py --prompt "Hello, world!"

# With specific model
python ollama_token_visualizer.py --model llama3:latest --prompt "Explain AI"

# List available models
python ollama_token_visualizer.py --list-models
```

### 2. LLaMA.cpp Probability Visualizer (Advanced)

**First, start the llama.cpp server:**
```bash
./start_llamacpp_server.sh
```

**Then run the visualizer:**
```bash
# Basic - shows token probabilities
python llamacpp_probability_visualizer.py --prompt "The sky is"

# Show top 10 alternatives per token
python llamacpp_probability_visualizer.py --prompt "AI is" --n-probs 10

# Decision tree view
python llamacpp_probability_visualizer.py --prompt "Once upon a time" --tree

# Adjust temperature
python llamacpp_probability_visualizer.py --prompt "Hello" --temperature 1.0
```

See [LLAMACPP_GUIDE.md](LLAMACPP_GUIDE.md) for detailed llama.cpp usage.

## Examples

```bash
# Quick question
python ollama_token_visualizer.py -p "What is the capital of France?"

# Code generation
python ollama_token_visualizer.py -m codellama -p "Write a Python function to calculate fibonacci numbers"

# Creative writing
python ollama_token_visualizer.py -m mistral -p "Tell me a short story about a robot"
```

## Output Sections

The visualizer displays four sections:

1. **PROMPT** - Your input prompt
2. **TOKENS** - Color-coded individual tokens as they stream in
3. **STATISTICS** - Performance metrics (duration, tokens/second)
4. **COMPLETE TEXT** - The final assembled text output

## Troubleshooting

### "Error connecting to Ollama"
- Make sure Ollama is running: `ollama serve`
- Check that Ollama is accessible at `http://localhost:11434`

### "No models found"
- Pull a model first: `ollama pull llama2`
- List available models: `ollama list`

### Slow generation
- Smaller models (like `llama2:7b`) are faster than larger ones
- Performance depends on your hardware (CPU/GPU)

## How It Works

The tool uses Ollama's streaming API to receive tokens in real-time. Each token (which could be a word, part of a word, or punctuation) is displayed with a unique color as it arrives, giving you insight into how the LLM constructs its response piece by piece.

## License

MIT
